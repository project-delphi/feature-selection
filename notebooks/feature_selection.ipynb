{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is trying out different strategies for feature selection based on sklearn as well as a few neural approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    RFE,\n",
    "    RFECV,\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    SelectPercentile,\n",
    "    VarianceThreshold,\n",
    "    chi2,\n",
    "    f_classif,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset to parent data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f ../data/dorothea.zip ]; then wget -P ../data/ https://archive.ics.uci.edu/static/public/169/dorothea.zip && unzip ../data/dorothea.zip -d ../data/; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, features_file, targets_file):\n",
    "        self.features_file = features_file\n",
    "        self.targets_file = targets_file\n",
    "        self.df = None\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        data = []\n",
    "        with open(self.features_file, 'r') as f:\n",
    "            for line in f:\n",
    "                active_features = line.strip().split()\n",
    "                data.append(pd.Series({int(feature): 1 for feature in active_features}))\n",
    "        features = pd.concat(data, axis=1).T.fillna(0).sort_index(axis=1)\n",
    "        targets = pd.read_csv(self.targets_file, header=None, names=[\"target\"])\n",
    "        self.df = pd.concat([features, targets], axis=1)\n",
    "    \n",
    "    def __call__(self):\n",
    "        self._preprocess()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangler = DataPreprocessor('../data/DOROTHEA/dorothea_train.data', '../data/DOROTHEA/dorothea_train.labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('target', axis=1), df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number_unique_values = {column: X[column].unique() for column in X.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_by_columns = [value for _, value in feature_number_unique_values.items()]\n",
    "tuples = [tuple(np.sort(arr)) for arr in unique_values_by_columns]\n",
    "\n",
    "# Count the frequencies\n",
    "frequency_table = Counter(tuples)\n",
    "\n",
    "# Convert Counter to DataFrame\n",
    "df = pd.DataFrame.from_records(list(frequency_table.items()), columns=['Array', 'Frequency'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that each feature takes binary values 1 or 0, at least 1 of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the target `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drug discovery target `y` is also binary, taking values `-1` and `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Based Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features using univariate statistical tests of teh relationship between each feature and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Feature Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Threshold\n",
    "\n",
    "Let's see a histogram of the variance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = X.var()\n",
    "\n",
    "plt.hist(variances, bins='auto', log=True)\n",
    "plt.title('Histogram of Variances')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_variances = np.log(variances + 1e-9)\n",
    "\n",
    "plt.hist(log_variances, bins='auto', log=True)\n",
    "plt.title('Histogram of Log Variances', )\n",
    "plt.xlabel('Log Variance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(variances, bins='auto', density=True, cumulative=True, histtype='step', alpha=0.8)\n",
    "plt.title('CDF of Variances')\n",
    "plt.xlabel('Log Variance')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate the median of variances\n",
    "median = np.median(variances)\n",
    "\n",
    "# Plot the median as a dotted line\n",
    "plt.axvline(median, color='b', linestyle='dotted', linewidth=2, label=f'Median Variance: {median:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(variances)\n",
    "plt.title('Boxplot of Variances')\n",
    "plt.ylabel('Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_variances, bins='auto', density=True, cumulative=True, histtype='step', alpha=0.8)\n",
    "plt.title('CDF of Log Variances')\n",
    "plt.xlabel('Log Variance')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate the median of log_variances\n",
    "median = np.median(log_variances)\n",
    "\n",
    "# Plot the median as a dotted thick blue line\n",
    "plt.axvline(median, color='b', linestyle='dotted', linewidth=2, label=f'Median Variance: {median:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(variances)\n",
    "plt.title('Boxplot of Variances')\n",
    "plt.ylabel('Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_cutoff = 0.01\n",
    "selector = VarianceThreshold(threshold=variance_cutoff)\n",
    "selected_features = selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based Filtering: Feature To Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful to one hot encode the target y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = OneHotEncoder(sparse_output=False).fit_transform(y.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification based univariate feature selection using SelectBestK, SelectPercentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out slow running block\n",
    "# X_new = SelectKBest(mutual_info_classif, k=2).fit_transform(X, y)\n",
    "# X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectPercentile(f_classif, percentile=0.01).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectPercentile(chi2, percentile=0.01).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out slow running block\n",
    "# X_new = SelectPercentile(mutual_info_classif, percentile=0.01).fit_transform(X, y)\n",
    "# X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE is the same as Seqential Feature Selection with backward elimination. It recursively removes the weakest feature (according to some model ranking - often coefficients listed in `coef_`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing RFE using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty=\"l2\")\n",
    "selector = RFE(estimator=logreg, n_features_to_select=88110, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# Print the mask of selected features\n",
    "print(selector.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validate the number of features to eliminate in RFE routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_features_to_select = 88110  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE with Support vector classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_features_to_select = 88110  # Minimum number of features to consider\n",
    "clf = RandomForestClassifier()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_l1 = LogisticRegression(penalty='l1', solver='liblinear').fit(X, y)\n",
    "model = SelectFromModel(logistic_l1, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long running process\n",
    "# knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# sfs_backward = SequentialFeatureSelector(\n",
    "#     knn, n_features_to_select = X.shape[1] - 5, direction=\"backward\"\n",
    "# ).fit(X, y)\n",
    "\n",
    "# print(\n",
    "#     \"Number Features selected by forward sequential selection: \"\n",
    "#     f\"{len([sfs_forward.get_support()])}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(dual=\"auto\", penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Permutation Importance Based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for permutation importance\n",
    "class PermutationImportanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, scoring, n_repeats=10, random_state=0, k=10):\n",
    "        self.model = model\n",
    "        self.scoring = scoring\n",
    "        self.n_repeats = n_repeats\n",
    "        self.random_state = random_state\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        self.importance = permutation_importance(self.model, X, y, scoring=self.scoring, n_repeats=self.n_repeats, random_state=self.random_state)\n",
    "        self.selector = SelectKBest(k=self.k).fit(X, self.importance.importances_mean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "# Assuming X and y are your data\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', PermutationImportanceTransformer(LogisticRegression(penalty='l2', solver='liblinear'), scoring='accuracy', k=10)),\n",
    "    ('classification', LogisticRegression(penalty='l2', solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Fit and use the pipeline\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features\n",
    "X = X.to_numpy()\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Define the encoder dimension\n",
    "encoder_dim = 10  # change this to your desired number of features\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Sequential([\n",
    "    Dense(encoder_dim, input_shape=(n_features,), activation='relu'),\n",
    "    Dense(n_features, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Fit the autoencoder model\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[0].output)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done in Keras above and PyTorch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your data\n",
    "# Convert X to a PyTorch tensor\n",
    "X_tensor = torch.from_numpy(X.values).float()\n",
    "\n",
    "# Define the number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Define the encoder dimension\n",
    "encoder_dim = 10  # change this to your desired number of features\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, encoder_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Train the autoencoder\n",
    "for epoch in range(50):\n",
    "    autoencoder.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = autoencoder(X_tensor)\n",
    "    loss = criterion(outputs, X_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Use the encoder to transform the data\n",
    "autoencoder.eval()\n",
    "X_transformed = autoencoder.encoder(X_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your data\n",
    "# Define the number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Define the encoder dimension\n",
    "encoder_dim = 10  # change this to your desired number of features\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Sequential([\n",
    "    Dense(encoder_dim, input_shape=(n_features,), activation='relu'),\n",
    "    Dense(n_features, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Fit the autoencoder model\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[0].output)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = jnp.array(X.values)\n",
    "y = jnp.array(y.values)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(features=1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense(x)\n",
    "        return nn.sigmoid(x)\n",
    "\n",
    "def loss_fn(params, model, x, y, l1_reg):\n",
    "    y_pred = model.apply(params, x)\n",
    "    loss = jnp.mean(jnp.square(y_pred - y))\n",
    "    l1_penalty = l1_reg * jnp.sum(jnp.abs(params))\n",
    "    return loss + l1_penalty\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(learning_rate=0.001)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = Model()\n",
    "params = model.init(jax.random.PRNGKey(0), X)\n",
    "opt_state = optimizer.create(params)\n",
    "\n",
    "# Define a training step\n",
    "def train_step(opt_state, x, y, l1_reg):\n",
    "    params = optimizer.target(opt_state)\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, model, x, y, l1_reg)\n",
    "    opt_state = optimizer.update(grads, opt_state)\n",
    "    return opt_state, loss\n",
    "\n",
    "# Train the model\n",
    "l1_reg = 0.01  # adjust this to your desired level of regularization\n",
    "for _ in range(1000):\n",
    "    opt_state, loss = train_step(opt_state, X, y, l1_reg)\n",
    "\n",
    "# Use the model to make predictions\n",
    "params = optimizer.target(opt_state)\n",
    "y_pred = model.apply(params, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TabTransformer based selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming X and y are your data\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert categorical features to integers\n",
    "for col in X_train.columns[X_train.dtypes == 'object']:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Define the TabNet model\n",
    "clf = TabNetClassifier()\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(\n",
    "  X_train.values, y_train.values,\n",
    "  eval_set=[(X_train.values, y_train.values), (X_test.values, y_test.values)],\n",
    "  eval_name=['train', 'valid'],\n",
    "  eval_metric=['auc'],\n",
    "  max_epochs=1000 , patience=20,\n",
    "  batch_size=256, virtual_batch_size=128,\n",
    "  num_workers=0,\n",
    "  weights=1,\n",
    "  drop_last=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "preds = clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
